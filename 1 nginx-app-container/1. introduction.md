# what is NGINX

web server
reverse proxy
load balancing
backend routing
caching
api gateway

# why i need it

# what is layer4 and layer7

layer4 -> tcp/ip stack only
source ip, source port, destination ip, destination port, simple packet inspection
layer7 -> http/gRPC etc...
access of the content
url, headers, and cookies
require decryption

# what is proxy

# TLS -> for establish end-to-end encryption, use symmetric encryption

TLS termination cases
TLS passthrough
Architecture of NGINX
Master Process
|
| spins up a worker process per CPU core by default
|
w1 w2 w3 child process
|
Kernel

Listener — When the backend applications listens on a particular IP and port it creates a socket. A socket is not a connection but a place to connect to. Just like a wall socket, where you can plug in connections. Many connections can be connected to the socket. The listener is the process where the socket lives.
Acceptor — With a socket in hand, the backend can call the OS function accept passing this socket to accept any available connections on this socket. The accept function returns a file descriptor representing the connection. Connections have to be actively accepted by the application in order to serve clients. Otherwise the connections will remain in the OS acceptqueue unused. The acceptor is the thread or process that calls the accept function.
Reader — You can also call it worker too, this is where the work happens. When we have a connection, the backend application is also responsible to read data sent to this connection otherwise the buffer allocated by the OS for this connection will fill up and client won’t be able to send any more data. I call the process that reads the TCP stream on the connection the Reader. The reader passes in the file descriptor (connection) to read the stream and act on it.
TCP Stream vs Requests — TCP is a streaming protocol. Let us translate what that means for our HTTP web server. When you send a GET request through Axios from the frontend, Axios will create a TCP connection (if one doesn’t exist) and build out the HTTP request consisting of the method, protocol version, headers , URL parmaters etc. The HTTP request is well defined it has a start and an end. But guess what? the TCP stream is just raw bytes of data, so the reader is responsable to read all the stream and “look” for requests, oh here is a start of the request, let me continue reading, ok I see headers, URL, and yeah that is the end of the request. This collection of bytes is now translates to a logical request (sometimes called message because we like to confuse people) that request is delivered to the application layer 7 for processing. So it is not as easy we all think. This is true for any layer 7 protocol that uses TCP, HTTP/2, gRPC, SSH. This parsing can take a toll on whatever thread does it.

architecture patterns on how threads and connections are managed

1. Single Threaded Architecture.
2. Multiple Threads Single Acceptor Architecture
3. Multiple Threads Multiple Acceptors Architecture
4. Multiple Threads with Message-based Load Balancing Architecture
5. Multiple Threads with Socket Sharding

<!-- docker run --name nginx -p 80(accessing address from host machine):80(container running) --hostname ng1 -d nginx -->

```code
docker inspect container-name
```

run nginx

```code
docker run --name nginx -p 80:80 --hostname ng1 -d nginx
```

change html for nginx

```code
docker run --name nginx --hostname ng1 -p 80:80 -v ./:/usr/share/nginx/html -d nginx
```

build from docker file to create nodeapp

```code
docker build ./ -t nodeapp
```

just for testing purpose

```code
docker run -p 8080:8080 -d nodeapp
docker run -p 8080:8080 --hostname testnode -d nodeapp
```

create 3 new nodeapp

```code
docker run --hostname nodeapp1 --name nodeapp1 -d nodeapp
docker run --hostname nodeapp2 --name nodeapp2 -d nodeapp
docker run --hostname nodeapp3 --name nodeapp3 -d nodeapp
```

change config to use loadbalancer with multiple nodeapp

```code
docker run --name nginx --hostname ng1 -p 80:8080 -v ./nginx.conf:/etc/nginx/nginx.conf -d nginx
```

```code
docker network create backendnet
docker network connect backendnet nodeapp1
docker network connect backendnet nodeapp2
docker network connect backendnet nodeapp3
docker network connect backendnet nodeapp4
docker network connect backendnet nodeapp5
docker network connect backendnet nginx
```

create ng2

```code
docker run --name n2 --hostname ng2 -p 81:8080 -v ./nginx.conf:/etc/nginx/nginx.conf -d nginx
```

add ng2 also
```code
docker network connect backendnet ng2
docker restart ng2
```

```code
docker run -p 80:80 -d httpd
```

```code
docker build . -t nhttd
docker run --name s1 nhttd
docker run --name s2 nhttd
```

```code
docker network create backend --subnet 10.0.0.0/24
docker network connect backend s1
docker network connect backend s2
```

```code
docker network disconnect bridge s1
docker network disconnect bridge s2
```

```code 
docker network ls
docker network create frontend --subnet 10.0.1.0/24
docker network disconnect backend s2
docker network connect frontend s2
docker run --name gw --network backend -d nhttpd
docker network connect frontend gw

docker stop s1
docker stop s2
docker rm s1 s2

docker run --name s1 --network backend --cap-add=NET_ADMIN -d nhttpd
docker run --name s2 --network frontend --cap-add=NET_ADMIN -d nhttpd

docker exec -it gw bash
ip route add 10.0.0.0/24 via 10.0.1.3
traceroute 10.0.0.2

docker exec -it s1 bash
ip route add 10.0.1.0/24 via 10.0.0.3
ping 10.0.1.2
curl 10.0.1.2

docker exec -it s2 bash
ping 10.0.0.2
curl 10.0.0.2
traceroute 10.0.0.2
```